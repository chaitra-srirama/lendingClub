---
title: "IDS 572 Assignment 1"
author: "Britney Scott, Abdullah Saka, Shourya Narayan, Chaitra Srirama"
date: "2/8/2020"
output:
  html_document: default
  pdf_document: default
---

# Background

LendingClub is an American peer-to-peer lending company that offers an online platform for matching borrowers seeking loans and lenders looking to make an investment. It provides an online platform which enables borrowers and investors to pair with each other. Both individuals and institutions can participate as investors if they satisfy financial stability standards put forth by LendingClub ("Lending Club" 5-6). 

LendingClub is appealing to investors because they can choose how much to fund each borrow at $25 increments ("Alternative Investments"). Investors who hold diverse portfolios with LendingClub historically have a positive return ("Your Return"). Investors have control over the amount of risk they choose to take on, and have access to risk grades from LendingClub. LendingClub grades all loans from A to G, with each grade being further divided into five subgrades based on factors such as the borrower's FICO score and loan amount ("LendingClub" 8-9). Because the notes have the status of unsecured creditors, there is a risk that investors may lose all or part of the money if LendingClub becomes insolvent, even if the ultimate borrower continues to payback money ("LendingClub" 12).

Interest rates vary 6.03% to 26.06% between different types of loans and depend on a large number of factors regarding the borrower ("LendingClub" 3). A background check performed by LendingClub takes into consideration the borrower’s credit score, credit history, income, and other attributes which help to determine the loan grade. The minimum credit criteria for borrowers to obtain a loan is:

* A minimum FICO score of 660
* Below 35% debt-to-income ratio excluding mortgages
* Good debt-to-income ratio including mortgages
* At least 36 months of credit history
* At least two open accounts
* No more than 6 recent (last 6 months) inquiries ("LendingClub" 6)

LendingClub makes money by charging fees to both the borrowers and the lenders. Borrowers pay an origination fee when the loan is given, and investors pay a service fee of 1% ("LendingClub" 10). LendingClub also charges investors collection fees when payments are missed by the borrower, if applicable ("Interest Rates and Fees").

# Data Exploration

The analysis will begin with some exploration of the provided data. The output vatiable indicates whether a loan defaulted or not.

```{r setup, echo=FALSE, include=FALSE}

lcdf <- read.csv('C:/Users/chait/Desktop/Data Mining/Assignment 1/lcData4m.csv')
library(ggplot2)
#library(tidyverse)
library(dplyr)
library(lubridate)
library(ggcorrplot)
library(magrittr)
library(rpart)
library(ROCR)
library(C50)
library(knitr)
library(caret)
library(e1071)
library(gridExtra)

attach(lcdf)
knitr::opts_chunk$set(comment = NA)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=5, fig.height=3)
```

There are 13,652 defaulted loans in the dataset and 78,972 loans which were fully paid. About 14.74 per cent of the data represents defaulted loans. 

```{r, echo=FALSE, fig.align='center'}
#What is the proportion of defaults in the data?
#summary(lcdf$loan_status)
dat <- data.frame(table(lcdf$status))
dat
names(dat) <- c("LoanStatus","Count")
ggplot(data=dat, aes(x=LoanStatus, y=Count, fill=LoanStatus)) + geom_bar(stat="identity") + xlab("Loan Status") + ylab("Total Loans") + labs(fill = "Loan Status")

```

Loan grade seems to correlate with loan defaulting, as evident in the following graph. This is to be expected, because loans with better grades such as 'A' and 'B' are less risky. Only 5.17 per cent of the A grade loans defaulted as opposed to 45.07 per cent of G grade loans, the lowest grade.

```{r, echo=FALSE, fig.align='center'}
#How does default rate vary with loan grade? 
#table(lcdf$loan_status, lcdf$grade)
dat <- data.frame(table(lcdf$loan_status, lcdf$grade))
names(dat) <- c("LoanStatus","Grade", "count")
ggplot(data=dat, aes(x=Grade, y=count, fill=LoanStatus)) + geom_bar(stat="identity") + xlab("Loan Grade") + ylab("Total Loans") + labs(fill = "Loan Status")
knitr::opts_chunk$set(fig.width=9, fig.height=4)
```

Taking a closer look at the subgrades, we see even more variation. Within the 'B' rating, for example, 8.96 per cent of B1 rated loans, 11.39 of B3 rated loans, and 14.46 per cent of B5 rated loans defaulted. This, again, is to be expected as the ratings of the loans get progressively lower.

```{r, echo=FALSE, fig.align='center'}
#Does it vary with sub-grade?
#table(lcdf$loan_status, lcdf$sub_grade)
dat <- data.frame(table(lcdf$loan_status, lcdf$sub_grade))
dat
names(dat) <- c("LoanStatus","SubGrade", "Count")
ggplot(data=dat, aes(x=SubGrade, y=Count, fill=LoanStatus)) + geom_bar(stat="identity") + xlab("Loan Sub Grade") + ylab("Total Loans") + labs(fill = "Loan Status")
knitr::opts_chunk$set(fig.width=5, fig.height=3)
```

The number of loans within each grade category vary quite a bit, with B loans having the highest count of 29,523 loans. E, F and G categories contain only 3,309, 463, and 71 loans respectively. 

The following plots show the number of loans in each grade, as well as in each subgrade. The large variation is evident.

```{r, echo=FALSE, fig.align='center'}
#How many loans are there in each grade?
dat <- data.frame(table(lcdf$grade))
names(dat) <- c("Grade", "Count")
ggplot(data=dat, aes(x=Grade, y=Count, fill=Grade)) + geom_bar(stat="identity") + xlab("Loan Grade") + ylab("Total Loans") + theme(legend.position = "none")
knitr::opts_chunk$set(fig.width=8, fig.height=3)
```
```{r, echo=FALSE, fig.align='center'}
#How many in each sub-grade?
dat <- data.frame(table(lcdf$sub_grade))
names(dat) <- c("SubGrade", "Count")
ggplot(data=dat, aes(x=SubGrade, y=Count, fill=SubGrade)) + geom_bar(stat="identity") + xlab("Loan Sub Grade") + ylab("Total Loans") + theme(legend.position = "none")
```

Examining the average loan amount in each grade in the data. As observed from the plot and the table, the average loan amount for each grade decreases as the grade worsens. Thi sis to be expected as investors would invest in lower amount of loans as the the grade worsens. 

```{r, echo=FALSE}
#Do loan amounts vary by each grade? The average loan amount per each grade and subgrade.
lcdf %>% 
  group_by(grade) %>% 
  summarise(Average = mean(loan_amnt))

ggplot(lcdf, aes(x=grade, y=loan_amnt, fill=grade)) + geom_boxplot()

```

Interest rate varies drastically by the grade of the loan, as shown in the table below The same applies when subgrades are examined, and a steady increase in the interest rate can be seen with each step lower in subgrade. This is to be expected, since a lower grade indicates higher risk and therefore requires a higher rate of return.

```{r, echo=FALSE, fig.align='center'}
#Does interest rate vary by grade?
lcdf$int_rate2 = as.numeric(gsub("%", "", lcdf$int_rate))
x <- lcdf %>% 
  group_by(grade) %>% 
  summarise(average = mean(int_rate2))

knitr::kable(x, align = c('c', 'c'), col.names=c("Loan Grade","Average Interest Rate"))

```

```{r, echo=FALSE, fig.align='center', message=FALSE}
#Does interest rate vary by subgrade?
x <- lcdf %>% 
  group_by(sub_grade) %>% 
  summarise(average = mean(int_rate2))

knitr::kable(x, align = c('c', 'c'), col.names=c("Loan Sub Grade","Average Interest Rate"))
knitr::opts_chunk$set(fig.width=5, fig.height=3)
```

The following boxplot helps to illustrate the increase in interest rate as the grade of the loan worsens.

```{r, echo=FALSE, fig.align='center', message=FALSE}
ggplot(lcdf, aes(x=grade, y=int_rate2, fill=grade)) + geom_boxplot(outlier.shape = NA) + xlab("Loan Grade") + ylab("Interest Rate") + theme(legend.position = "none")
knitr::opts_chunk$set(fig.width=7, fig.height=3)
```

It's also important to look at what people are borrowing their money for. The vast majority of the loans in the dataset are for debt consolidation, with credit card refinancing in second place. The above graph shows the count of each purpose, as well as the proportion of that type of loan that defaulted. Credit card refinancing has the lowest default rate in the dataset. The highest default rate is for green loans, but there are only 59 total in the dataset of this category. 

```{r, echo=FALSE, fig.align='center'}
#What are people borrowing money for (purpose)? 
dat <- data.frame(table(lcdf$title, lcdf$loan_status))
names(dat) <- c("Purpose", "Outcome", "Count")
ggplot(data=dat, aes(x=Purpose, y=Count, fill=Outcome))+geom_bar(stat="identity")+ scale_x_discrete(labels = abbreviate) + labs(fill = "Loan Status")
knitr::opts_chunk$set(fig.width=7, fig.height=3)
```

The amount of money given varies depending on the purpose of the loan. The following boxplot illustrates these differences well. Vacation loans have the smallest average amount, while credit card refinancing loans are typically quite large. 

```{r, echo=FALSE, fig.align='center'}
#Average Amount of Loans by the purpose.
#lcdf %>% 
#  group_by(title) %>% 
#  summarise(average = mean(loan_amnt))
ggplot(lcdf, aes(x=title, y=loan_amnt, fill=title)) + geom_boxplot(outlier.shape = NA) + scale_x_discrete(labels = abbreviate) + xlab("Loan Purpose") + ylab("Loan Amount") + labs(fill = "Loan Purpose") 
```
We also observe the purpose of loan amount by eacg grade. As observed highest number of loans seen in Debt consolidation in all grades. Lowest number of loans seen in Green Loans in all grades.
```{r, echo=FALSE}
#Purpose of loan amount by grade
table(lcdf$title, lcdf$grade)
```

Next, we can calculate annual return for each loan using the following equation:  
$((Total Payment - Funded Amount)/Funded Amount)*(12/36)*100$

Comparing the average return to the average interest rate, the two are negatively correlated. Across the different loan grades, as the interest rate increases, the annual return is decreasing. The average annual return of some of the lowest graded loans is even negative. This makes sense since we know the loans with lower grades are more likely to default. For the most part, the annual return increases as subgrade worsens too. The difference between interest rate and annual return is the smallest for the loans with better grades.

```{r, echo=FALSE}
  #Calculate rate of annual return
  lcdf$annRet_percent = ((lcdf$total_pymnt-lcdf$funded_amnt)/lcdf$funded_amnt)*(12/36)*100
  
  x <- lcdf %>% 
    group_by(grade) %>% 
  summarise(AverageInterestRate = mean(int_rate2), AverageAnnualReturn = mean(annRet_percent ), Difference=AverageInterestRate-AverageAnnualReturn)
  
knitr::kable(x, align = c('c', 'c', 'c', 'c'), col.names=c("Loan Grade","Average Interest Rate", "Average Annual Return", "Difference"))
```

```{r, echo=FALSE}
x <- lcdf %>% 
  group_by(sub_grade) %>% 
  summarise(AverageInterestRate = mean(int_rate2), AverageAnnualReturn = mean(annRet_percent ), Difference=AverageInterestRate-AverageAnnualReturn)
knitr::kable(x, align = c('c', 'c', 'c', 'c'), col.names=c("Loan Sub Grade","Average Interest Rate", "Average Annual Return", "Difference"))
```

# Variable Exclusion and Manipulation

We chose to add in a few additional derived attributes. 

* Proportion of satisfactory bankcard accounts
* Proportion of open accounts that are satisfactory
* Ratio of amount funded by investor to total loan amount
* Ratio of funded amount to annual income of borrower
* Monthly debt percentage of borrower
* Ratio of open acocunts to total accounts

Boxplots for all of these attributes show how they vary between loans that were paid off and ones that defaulted.

```{r, echo=FALSE}
#Derived attributes

#Proportion of satisfactory bankcard accounts 
lcdf$propSatisBankcardAccts <- ifelse(lcdf$num_bc_tl>0, lcdf$num_bc_sats/lcdf$num_bc_tl, 0)

#Proportion of Proportion of open accounts that are satisfactory
lcdf$PropSatAcc <- ifelse(lcdf$total_acc>0, lcdf$num_sats/lcdf$total_acc, 0)

#Ratio of amount funded by investor to total loan amount
lcdf$PropFunAmt <- ifelse(lcdf$loan_amnt>0, lcdf$funded_amnt_inv/lcdf$loan_amnt, 0)

#Ratio of funded amount to annual income of borrower
lcdf$PropFundvsInc<- ifelse(lcdf$annual_inc>0, lcdf$funded_amnt_inv/lcdf$annual_inc, 0)

#Monthly debt percentage of borrower- Gives insight to the financial burden of the loan amount on the borrower every month
lcdf$mnthDebt <- (lcdf$installment/(lcdf$annual_inc/12))*100

#Ratio of open accounts to total accounts
lcdf$OpenRatio <- ifelse(lcdf$total_acc>0, lcdf$open_acc/lcdf$total_acc, 0)
```

```{r, echo=FALSE, fig.align='center'}
#Boxplots for all of the derived variables

plot1<-ggplot(lcdf, aes(x=lcdf$loan_status, y=lcdf$PropSatAcc, fill = lcdf$loan_status)) + geom_boxplot() + xlab("Loan Status")+ ylab(expression(atop("Proportion of Satisfactory Accounts", paste("to Total Open Accounts")))) + theme(legend.position = "none")+ coord_flip()

grid.arrange(plot1, plot2, ncol=2)

plot1 <- ggplot(lcdf, aes(x=lcdf$loan_status, y=lcdf$PropFunAmt, fill = lcdf$loan_status)) + geom_boxplot() + xlab("Loan Status") + ylab(expression(atop("Ratio of Amount Funded by Investor", paste("to Total Loan Amount")))) + theme(legend.position = "none")+ coord_flip()

plot2 <- ggplot(lcdf, aes(x=lcdf$loan_status, y=lcdf$PropFundvsInc, fill = lcdf$loan_status)) + geom_boxplot() + xlab("Loan Status") + ylab(expression(atop("Ratio of Funded Amount to Annual", paste("Income of Borrower")))) + theme(legend.position = "none")+ coord_flip()

grid.arrange(plot1, plot2, ncol=2)

plot1 <- ggplot(lcdf, aes(x=lcdf$loan_status, y=lcdf$mnthDebt, fill = lcdf$loan_status)) + geom_boxplot() + xlab("Loan Status") + ylab(expression(atop("Monthly Debt Percentage of", paste("Borrower")))) + theme(legend.position = "none")+ coord_flip()

plot2 <- ggplot(lcdf, aes(x=lcdf$loan_status, y=lcdf$OpenRatio, fill = lcdf$loan_status)) + geom_boxplot() + xlab("Loan Status") + ylab(expression(atop("Ratio of Open Accounts to", paste("Total Accounts")))) + theme(legend.position = "none")+ coord_flip()

grid.arrange(plot1, plot2, ncol=2)
```


```{r, echo=FALSE}
#Removing NA values higher than 60%
loan_data <- lcdf[, -which(colMeans(is.na(lcdf)) > 0.6)] 

#Remove unnecessary columns for data leakage
loan_data <- loan_data %>% select(-c(fico_range_low, fico_range_high, last_fico_range_high, last_fico_range_low, num_tl_120dpd_2m, num_tl_30dpd, acc_now_delinq, funded_amnt_inv, term, emp_title, pymnt_plan, title, zip_code, addr_state, out_prncp, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_int, total_rec_late_fee, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, last_credit_pull_d, policy_code, debt_settlement_flag, hardship_flag, issue_d, earliest_cr_line, application_type))

```

We decided to remove all of the attributes with more than 60% missing values. This decreases the number of independent variables from 150 to 92. 

Next, some variables which may cause leakage need to be removed. These are variables which have been updated after the loan was given. For example, FICO score is updated every time an individual goes through a credit check, so all variables including FICO score have been removed. Other variables which are updated include total payment and interest payments received to date. After removing these unnecessary columns, the total number of independent variables decreases further to 60 columns.

Next, missing values must be addressed. For some columns, the absence of a value is meaningful. For example, a missing value for months since recent inquiry indicates that there has not been an inquiry. We cannot fill these fields with a zero, as that would indicate a very recent inquiry. For such columns, we filled the missing values with a number much higher than the maximum value for the column. Other columns with which we used this approach include months since oldest installment account opened, months since most rencent bankcard account opened, and months since last delinquency.

In other cases, the NA truly indicates a missing value. For these columns, we replaced the missing values with the median for that column. We used this approach for revolving line utilization rate, total open to buy on revolving bankcards, ratio of current balance to credit limit for all bankcard accounts, and percentage of bankcards over 75% percent of their limit.

```{r, echo=FALSE}
#Replacing missing values

#summary(loan_data$mths_since_last_delinq)
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_last_delinq = 500))
loan_data<- loan_data %>% tidyr::replace_na(list(revol_until=median(loan_data$revol_until, na.rm=TRUE)))
loan_data<- loan_data %>%  tidyr::replace_na(list(bc_open_to_buy=median(loan_data$bc_open_to_buy, na.rm=TRUE)))
loan_data<- loan_data %>%  tidyr::replace_na(list(bc_util=median(loan_data$bc_util, na.rm=TRUE)))
#summary(loan_data$mo_sin_old_il_acct)
loan_data<- loan_data %>%  tidyr::replace_na(list(mo_sin_old_il_acct = 1000))
#summary(loan_data$mths_since_recent_bc)
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_recent_bc = 1000))
#summary(loan_data$mths_since_recent_inq)
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_recent_inq = 100))
loan_data<- loan_data %>%  tidyr::replace_na(list(percent_bc_gt_75 =median(loan_data$percent_bc_gt_75 , na.rm=TRUE)))
```

# Decision Tree Models

The first step of building decision tree models is splitting the data between training and testing sets. We chose to split the data at a ratio of 70:30. 

### Information Model

For the first decision tree, we used the information method with a minimum split of 30 and a commplexity parameter of 0.0001. This performed at 91 per cent accuracy on the training set.

```{r, echo=FALSE}
# Set seed to produce same results
set.seed(9)
# Run a model using 'rpart' 
#change type of dependent variable as factor
loan_data$loan_status <- factor(loan_data$loan_status, levels=c("Fully Paid", "Charged Off"))
#nr= number of rows in the dataset loan_data
nr<-nrow(loan_data)
#Splitting data into training/testing sets using random sampling
#Training: 70%, Testing: 30%
# Here the sample function does the following: #Take samples from 1 to nr rows, with size - 70% of rows into the training data set. Here replace=FALSE meanswithout replacement.

trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) 

#Here the 70% of the sample is put into variable lcdfTrn and the remaining (-) is put into lcdfTst
lcdfTrn <- loan_data[trnIndex, ] 
lcdfTst <- loan_data[-trnIndex, ]

#We run the decision tree
#note: loan_status is the end classification required. data used= lcdfTrn, method used is the classification method.  
#read this: https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart

#split= information Method-> is nothing but it uses entropy/information gain as a measure of impurity. There are 2 types in rpart- gini and information gain 
#Gini Split / Gini Index	Favors larger partitions. Very simple to implement.
#Information Gain / Entropy	Favors partitions that have small counts but many distinct values.
#Read http://www.learnbymarketing.com/tutorials/rpart-decision-trees-in-r/


#Parms are the parameters used for building a decision tree.
#minsplit=the minimum number of observations that must exist in a node in order for a split to be attempted, i.e we need minimum of 30 observations to be present to make a split.
#minbucket provides the smallest number of observations that are allowed in a terminal node. If a split decision breaks up the data into a node with less than the minbucket, it won’t accept it.
#maxdepth parameter prevents the tree from growing past a certain depth / height
#The complexity parameter (cp) in rpart is the minimum improvement in the model needed at each node. The cp value is a stopping parameter. It helps speed up the search for splits because it can identify splits that don’t meet this criteria and prune them before going too far.The complexity parameter (cp) is used to control the size of the decision tree and to select the optimal tree size. It is used for pruning the tree.the default is 0.01
#If you take the approach of building really deep trees, the default value of 0.01 might be too restrictive, but we used 0.0001
#The higher the cp , the smaller the tree. A too small value of cp leads to overfitting and a too large cp value will result to a too small tree. Both cases decrease the predictive performance of the model.

lcDT1 <- rpart(loan_status ~., data=lcdfTrn, method="class", parms = list(split = "information"), control = rpart.control(minsplit = 30, cp=0.0001))

#veiw the tree
#rpart.plot::prp(lcDT1, type=2, extra=1)

#Accuracy
#the predict function provides predictions regarding the decision tree. Here it is of the form predict(tree, dataset, type of method used)
predTrn=predict(lcDT1, lcdfTrn, type='class')
Metric <- c("Training Accuracy") # create a colomn named training acuracy
Result <- c(round(mean(predTrn==lcdfTrn$loan_status),2)) #create another column result. Here we are taking prediction variable based on the loan status. All predicted values are converted to mean and the rounded off to 2 decimals
p <- as.data.frame(cbind(Metric, Result))
p
knitr::kable(p, align = c('c', 'c'))
```

This accuracy seemed rather high and leads to concerns about overfitting. After generating the model, we pruned it using a complexity parameter of 0.0003 in order to keep it at a manageable size and avoid small nodes which can lead to overfitting and lower accuracy on the validation data.

This pruned model performed well on the training data, and has 89 per cent accuracy. On the testing data, the model performance decreases to 82 per cent accuracy.

```{r, echo=FALSE}
#Pruning the tree form (prune.rpart(tree, cp))
lcDT1p<- prune.rpart(lcDT1, cp=0.0003)
```

The confusion matrix and accuracy of the first model after pruning for the training data:

```{r, echo=FALSE}
#Confusion table for training data
predTrn=predict(lcDT1p, lcdfTrn, type='class')
x <- confusionMatrix(predTrn,lcdfTrn$loan_status)
x$table

#Accuracy
Metric <- c("Training Accuracy")
Result <- c(round(mean(predTrn==lcdfTrn$loan_status),2)) #Accuracy after pruning
p <- as.data.frame(cbind(Metric, Result))
p
knitr::kable(p, align = c('c', 'c'))
```

The confusion matrix and performance metrics of the first model after pruning for the Testing data:

```{r, echo=FALSE}
#Confusion matrix for testing data
predTst1=predict(lcDT1p, lcdfTst, type='class')
x <- confusionMatrix(predTst1,lcdfTst$loan_status)
x$table

#Performance metrics
#precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually.
#Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. 

#v important read-> https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9
Metric <- c("Test Accuracy","Precision Score","Recall Score")
Result1 <- c(round(mean(predTst1==lcdfTst$loan_status),2), round(precision(predTst1,lcdfTst$loan_status),2), round(recall(predTst1,lcdfTst$loan_status),2))
p <- as.data.frame(cbind(Metric, Result1))
p
knitr::kable(p, align = c('c', 'c'), col.names=c("Metric", "Result"))
```

### Gini Model

Next, we created a second decision tree model using the same training and testing sets. All parameters were kept the same except for the method, which was changed from information to gini. Before pruning, this tree performed at 91 per cent accuracy on the training data.

```{r, echo=FALSE}
lcDT2 <- rpart(loan_status ~., data=lcdfTrn, method="class", parms = list(split = "gini"), control = rpart.control(minsplit = 30, cp=0.0001))

#Accuracy
predTrn=predict(lcDT2, lcdfTrn, type='class')
Metric <- c("Training Accuracy")
Result <- c(round(mean(predTrn==lcdfTrn$loan_status),2))
p <- as.data.frame(cbind(Metric, Result))
p
knitr::kable(p, align = c('c', 'c'))
```

Once again, we chose to prune the tree to avoid overfitting. This model performs slightly better on the training data than the first, with 89 per cent accuracy. On the testing data, however, the accuracy was 83 per cent, which is slightly higher than the first model.

The confusion matrix and accuracy of the second model after pruning for the training data:

```{r, echo=FALSE}
#Pruning the tree
lcDT2p<- prune.rpart(lcDT2, cp=0.0003)
#printcp(lcDT1p)

#Confusion table for training data
predTrn=predict(lcDT2p, lcdfTrn, type='class')
x <- confusionMatrix(predTrn,lcdfTrn$loan_status)
x$table

#Accuracy
Metric <- c("Training Accuracy")
Result <- c(round(mean(predTrn==lcdfTrn$loan_status),2))
p <- as.data.frame(cbind(Metric, Result))
p
knitr::kable(p, align = c('c', 'c'))
```

The confusion matrix and performance metrics of the second model after pruning for the testing data:

```{r, echo=FALSE}
#Confusion matrix for testing data
predTst2=predict(lcDT2p, lcdfTst, type='class')
x <- confusionMatrix(predTst2,lcdfTst$loan_status)
x$table

#Performance metrics
Metric <- c("Test Accuracy","Precision Score","Recall Score")
Result2 <- c(round(mean(predTst2==lcdfTst$loan_status),2), round(precision(predTst2,lcdfTst$loan_status),2), round(recall(predTst2,lcdfTst$loan_status),2))

p <- as.data.frame(cbind(Metric, Result2))
p
knitr::kable(p, align = c('c', 'c'), col.names=c("Metric", "Result"))
```

### C5.0 Model

Next, we chose to run a model using C5.0 to see how it compared to the rpart models. We selected confidence factor as 0.45 and the number of trials as 3. Overall, the C5.0 decision tree model performs slightly better than other models on the valdation data with 84 per cent accuracy.

The confusion matrix and accuracy of the C5.0 model for the training  data:

```{r, echo=FALSE, fig.align='center'}
# Run a model using 'C5.0'
c_tree <- C5.0(as.factor(lcdfTrn$loan_status) ~., data = lcdfTrn, method = "class", trials = 3, control=C5.0Control(CF=0.45,earlyStopping =FALSE))

#Confusion matrix for training data
predTrn=predict(c_tree, lcdfTrn, type='class')
x <- confusionMatrix(predTrn,lcdfTrn$loan_status)
x$table

#Accuracy
Metric <- c("Training Accuracy")
p <- as.data.frame(cbind(Metric, Result))
knitr::kable(p, align = c('c', 'c'))
```

The confusion matrix and performance metrics of the C5.0 model for the testing data:

```{r, echo=FALSE}
#Confusion matrix for testing data
predTst3=predict(c_tree,lcdfTst)
x <- confusionMatrix(predTst3,lcdfTst$loan_status)
x$table

#Performance metrics
Metric <- c("Test Accuracy","Precision Score","Recall Score")
Result3 <- c(round(mean(predTst3==lcdfTst$loan_status),2), round(precision(predTst3,lcdfTst$loan_status),2), round(recall(predTst3,lcdfTst$loan_status),2))

p <- as.data.frame(cbind(Metric, Result3))
p
knitr::kable(p, align = c('c', 'c'), col.names=c("Metric", "Result"))
```

Comparing the performance metrics of the three final models that we created, it's clear that the C5.0 model is superior. 

```{r, echo=FALSE}
Information <- Result1
Gini <- Result2
C5.0 <- Result3

p <- as.data.frame(rbind(Information, Gini, C5.0))
p
knitr::kable(p, col.names =c("Test Accuracy","Precision Score","Recall Score"), align = c('c', 'c','c'))
```

Finally, we checked which variables are more important for decision tree. The model looks at the improvement measure to each variable in its split. The values of these improvements are summed up, and are then scaled relative to the best variable.

These are the top ten attributes which carry more weight than other attributes (more statistically significant).

```{r, echo=FALSE}
imp_att<-as.data.frame(C5imp(c_tree,pct=FALSE))
imp_att<-head(imp_att,10)
knitr::kable(imp_att, align = c('c'))
```


ROC curves for each of the models are displayed below.

```{r, echo=FALSE, fig.align='center'}
par(mfrow=c(1,3))

#Information ROC Curve
score=predict(lcDT1p,lcdfTst, type="prob")[,"Charged Off"]
pred2=prediction(score, lcdfTst$loan_status, label.ordering = c("Fully Paid", "Charged Off"))
aucPerf1 <-performance(pred2, "tpr", "fpr")
plot(aucPerf1, main="Information")
abline(a=0, b= 1)

#Gini ROC Curve
score=predict(lcDT2p,lcdfTst, type="prob")[,"Charged Off"]
pred2=prediction(score, lcdfTst$loan_status, label.ordering = c("Fully Paid", "Charged Off"))
aucPerf2 <-performance(pred2, "tpr", "fpr")
plot(aucPerf2, main="Gini")
abline(a=0, b= 1)

#C5.0 ROC Curve
score=predict(c_tree,lcdfTst, type="prob")[,"Charged Off"]
pred2=prediction(score, lcdfTst$loan_status, label.ordering = c("Fully Paid", "Charged Off"))
aucPerf3 <-performance(pred2, "tpr", "fpr")
plot(aucPerf3, main="C5.0")
abline(a=0, b= 1)
```

###Random forest models 
  - install the 'randomForest' library first
```{r}
library('randomForest')

#for reproducible results, set a specific value for the random number seed
#set.seed(9)

#develop a model with 200 trees, and obtain variable importance
rfModel = randomForest(loan_status ~., data= lcdfTrn, ntree=50, importance=TRUE, na.action=na.roughfix)

predValid1_rf <- predict(rfModel1, lcdf1Tst, type = "class")


```
## References:

“Alternative Investments: How It Works.” LendingClub, LendingClub Corporation, 2020,  
www.lendingclub.com/investing/peer-to-peer.

“Interest Rates and Fees.” LendingClub, LendingClub Corporation, 6 Aug. 2019,  
www.lendingclub.com/investing/investor-education/interest-rates-and-fees.

“LendingClub.” 424B3, U.S. Securities and Exchange Commission, 30 Apr. 2014,  
www.sec.gov/Archives/edgar/data/1409970/000119312514173269/d719822d424b3.htm.

“Your Return: Three Key Factors.” LendingClub, LendingClub Corporation, 2020,  
www.lendingclub.com/investing/investment-performance.


