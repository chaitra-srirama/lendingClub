---
title: "GBM GLM WIP"
author: "Chaitra Srirama, Viharika Bharti, Aditya Gajula"
date: "2/26/2020"
output: html_document
---
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
lcdf <- read.csv('C:/Users/chait/Desktop/Data Mining/Assignment 1/lcData4m.csv')
library(ggplot2)
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggcorrplot)
library(magrittr)
library(rpart)
library(ROCR)
library(C50)
library(knitr)
library(caret)
library(e1071)
library(gridExtra)
library(rsample)
library(gbm)
library(glmnet)
library(purrr)
library(ROSE)
library(ranger)
```


#Derived attributes
```{r}
#annualized percentage return
lcdf$annRet <- ((lcdf$total_pymnt -lcdf$funded_amnt)/lcdf$funded_amnt)*(12/36)*100

#actual loan term in years
lcdf$last_pymnt_d <- paste(lcdf$last_pymnt_d, "-01", sep="")
lcdf$last_pymnt_d <- parse_date_time(lcdf$last_pymnt_d, "myd")
lcdf$actualTerm <- ifelse(lcdf$loan_status=="Fully Paid", as.duration(lcdf$issue_d %--% lcdf$last_pymnt_d)/dyears(1),3)

#actual Return with respect to actual Term
lcdf$actualReturn <- ifelse(lcdf$actualTerm>0, ((lcdf$total_pymnt - lcdf$funded_amnt)/lcdf$funded_amnt)*(1/lcdf$actalTerm),0)
lcdf$actualReturn <- ifelse(lcdf$actualTerm>0, ((lcdf$total_pymnt - lcdf$funded_amnt)/lcdf$funded_amnt)*(1/lcdf$actualTerm),0)

```


#Removing variables
```{r}

lcdf$int_rate= as.numeric(gsub("%", "", lcdf$int_rate))

#Removing NA values higher than 60%
loan_data <- lcdf[, -which(colMeans(is.na(lcdf)) > 0.6)] 

#(loan_data)

#Remove unnecessary columns for data leakage
loan_data <- loan_data %>% select(-c(fico_range_low, fico_range_high, last_fico_range_high, last_fico_range_low, num_tl_120dpd_2m, num_tl_30dpd, acc_now_delinq, funded_amnt_inv, term, emp_title, pymnt_plan, title, zip_code, addr_state, out_prncp, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_int, total_rec_late_fee, total_rec_prncp, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, last_credit_pull_d, policy_code, hardship_flag, issue_d, earliest_cr_line, application_type, revol_util))
                            
              
```

#Replacing missing values with appropriate values:

```{r}
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_last_delinq = 500))
#loan_data<- loan_data %>% tidyr::replace_na(list(revol_until=median(loan_data$revol_until, na.rm=TRUE)))
loan_data<- loan_data %>%  tidyr::replace_na(list(bc_open_to_buy=median(loan_data$bc_open_to_buy, na.rm=TRUE)))
loan_data<- loan_data %>%  tidyr::replace_na(list(bc_util=median(loan_data$bc_util, na.rm=TRUE)))
#summary(loan_data$mo_sin_old_il_acct)
loan_data<- loan_data %>%  tidyr::replace_na(list(mo_sin_old_il_acct = 1000))
#summary(loan_data$mths_since_recent_bc)
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_recent_bc = 1000))
#summary(loan_data$mths_since_recent_inq)
loan_data<- loan_data %>%  tidyr::replace_na(list(mths_since_recent_inq = 100))
loan_data<- loan_data %>%  tidyr::replace_na(list(percent_bc_gt_75 =median(loan_data$percent_bc_gt_75 , na.rm=TRUE)))
```

#Splitting into traning and test data for predicting Loan Status
```{r}
# Set seed to produce same results
set.seed(9)
lcdf_gbm<-loan_data

#converting loanstatus into 0s and 1s
lcdf_gbm$loan_status<-ifelse(lcdf_gbm$loan_status == "Fully Paid", 0, 1)
#lcdf_gbm$loan_status

#Split data into training, test subsets
nr<-nrow(lcdf_gbm)
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) 
lcdfTrn <- lcdf_gbm[trnIndex, ] 
lcdfTst <- lcdf_gbm[-trnIndex, ]

#oversampling the training dataset to get a balanced dataset.
os_lcdfTrn<-ovun.sample(loan_status~., data=as.data.frame(lcdfTrn), na.action = na.pass, method="over", p=0.5)$data
os_lcdfTrn%>%group_by(loan_status)%>% count() #count after oversampling

#both- undersampling and oversampling
bs_lcdfTrn<-ovun.sample(loan_status~., data=as.data.frame(lcdfTrn), na.action = na.pass, method="both", p=0.5)$data
bs_lcdfTrn%>%group_by(loan_status)%>% count() #count after sampling

```


#GBM Models to predict loan status.
1.  (a1) Develop gradient boosted models to predict loan_status.  Experiment with different parameter values, and identify which gives ‘best’ performance.  How do you determine ‘best’ performance? 

```{r}

#To get the best GBM Model with the help of parameter tuning:

paramGrid<-expand.grid( treeDepth= c(3, 5), minNodeSize= c(10, 30), bagFraction= c(.5, .8, 1), shrinkage = c(0.001, 0.01, 0.1), bestTree=0, best.iter=0, Misclas_res_Trn= 0, Accuracy_Trn=0, Misclas_res_Tst= 0, Accuracy_Tst=0)

paramGrid

for(i in 4:nrow(paramGrid)) {
gbm_paramTune<- gbm(formula=loan_status~., data=subset(os_lcdfTrn, select=-c(annRet, actualTerm, actualReturn)), distribution = 'bernoulli', n.trees= 500, interaction.depth= paramGrid$treeDepth[i], n.minobsinnode= paramGrid$minNodeSize[i], bag.fraction= paramGrid$bagFraction[i], shrinkage = paramGrid$shrinkage[i], train.fraction= 0.7, n.cores=NULL, cv.folds=5)  #use all available cores
#add best tree '

paramGrid$bestTree[i]<-which.min(gbm_paramTune$valid.error) 
best.iter[i] = gbm.perf(gbm_paramTune, method="cv")

#On Traning 
scores_gbm<-predict(gbm_paramTune, newdata=os_lcdfTrn, n.tree= best.iter, type="response")
fitted.scores <- ifelse(scores_gbm > 0.5,1,0)
Misclass_Trn<-(fitted.scores != os_lcdfTrn$loan_status)
Misclas_res_Trn[i]<-c(round(mean(Misclass_Trn),2))
Accuracy_Trn[i]<-1-Misclas_res_Trn[i]
Accuracy_Trn[i]

#On test
scores_gbm<-predict(gbm_paramTune, newdata=lcdfTst, n.tree= best.iter, type="response")
fitted.scores<-ifelse(scores_gbm > 0.5,1,0)
Misclass_Tst<-(fitted.scores != lcdfTst$loan_status)
Misclas_res_Tst[i]<-c(round(mean(Misclass_Tst),2))
Accuracy_Tst[i]<-1-Misclas_res_Tst[i]
Accuracy_Tst[i]
}


```

#Recreating the model with the least Misclassification error as follows:

```{r}
gbm_model<-gbm(loan_status~.,data=subset(os_lcdfTrn, select=-c(annRet, actualTerm, actualReturn)), distribution = 'bernoulli', n.trees= 500, n.minobsinnode=30, train.fraction= 0.7, bag.fraction=0.8, shrinkage=0.1, interaction.depth= 5, cv.folds=5, n.cores=NULL)

print(gbm_model)

#to check the best iteration
best.iter = gbm.perf(gbm_model, method="cv")
best.iter

#predicted values as per best iteration number.
scores_gbm<-predict(gbm_model, newdata=lcdfTst, n.tree= best.iter, type="response")
head(scores_gbm)

#fitting scores with a threshold of 0.5
fitted.scores <- ifelse(scores_gbm > 0.5,1,0)
#head(fitted.scores)

#confusion Matrix
conf_mat<-confusionMatrix(as.factor(fitted.scores),as.factor(lcdfTst$loan_status))
conf_mat$table
print("0 = Charged Off, 1= Fully Paid")

#Miscalssification Error
misclasserr<-(fitted.scores != lcdfTst$loan_status)
Result <- c(round(mean(misclasserr),2))
Result

#Accuracy
Accuracy<-1-Result
Accuracy

#Variable importance
var_imp<-as.data.frame(summary(gbm_model))
head(var_imp,15)

#performance ROC
pred_gbmM2=prediction(scores_gbm, lcdfTst$loan_status)
aucPerf_gbmM2 <-performance(pred_gbmM2, "tpr", "fpr")
plot(aucPerf_gbmM2)
abline(a=0, b= 1)

#AUC value.
auc=performance(pred_gbmM2, measure="auc") 
auc<-auc@y.values[[1]]
auc


```


#Q1 b1)Develop linear (glm) models to predict loan_status. Experiment with different parameter values and identify which gives ‘best’ performance. How do you determine ‘best’ performance? How do you handle variable selection? Experiment with Ridge and Lasso, and show how you vary these parameters, and what performance is observed.
```{r}

#VANILLA CASE GLM MODEL:

#converting the response variable to 0s and 1s:
os_lcdfTrn$loan_status <- as.integer(os_lcdfTrn$loan_status) #Converting to integer
is.integer(os_lcdfTrn$loan_status)
lcdfTst$loan_status <- as.integer(lcdfTst$loan_status) #Converting to integer
#anyNA(lcdfTst$loan_status)

#Building a logistic regression model as a vanilla case
log_model <- glm(loan_status~., data=subset(os_lcdfTrn, select=-c(annRet, purpose, actualTerm, actualReturn)), family=binomial)

print(log_model)

#Display regression coefficients- Variable Importance
coeffs<-as.data.frame(coef(log_model))
knitr::kable(coeffs)

#fitting the model on the test data.
prob_pred <- predict(log_model, newdata = lcdfTst, type = "response")
#head(prob_pred)

#Setting cut-off to be at 0.5
test_model <- model.matrix(loan_status ~., lcdfTst)[,-1]
probabilities <- log_model %>% predict(newx = lcdfTst)
pred.class <- ifelse(probabilities > 0.5, 1,0)
#head(pred.class)

#Accuracy
misClasificError <- mean(pred.class != lcdfTst$loan_status)
Accuracy<-(round(1-misClasificError,2))
Accuracy

#ROC Grpah
prob_pred <- predict(log_model, newdata = lcdfTst, type = "response")
pr <- prediction(prob_pred, lcdfTst$loan_status)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(0,1,lwd = 2, lty = 2)

#auc value
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

#https://rstudio-pubs-static.s3.amazonaws.com/293584_265e0868a3244c47b9f05b4b8223afa2.html
#Evaluating Model Performance
```
From the above we developed a logsitic regression model to predict loan status from the lending club data. As a vanilla case model, we got an accuracy of 85%, with an Area under the ROC curve to be 0.70. 


#Experimenting with different parameter values along with lasso and ridge regression
```{r}

#A)A)For lasso, alpha=1 

#Creating a data matrix
x <- model.matrix(loan_status~., os_lcdfTrn)[,-1]
y <- os_lcdfTrn$loan_status
#y

#to find the best lamda using cross validation with penalizing factor lasso alpha=1
cv.lasso <- cv.glmnet(x, y, data=subset(os_lcdfTrn, select=-c(annRet, purpose, actualTerm, actualReturn)), family="binomial", alpha=1 )
cv.lasso

#plot cross-validation results:
plot(cv.lasso)

#A1)fit model to traning data using cv_lasso$lambda.min i.e λ which gives minimum cross-validated error
model_lasso <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)

## Display regression coefficients
coef(model_lasso)

#model accuracy on traning data
x_train <- model.matrix(loan_status ~., lcdfTrn)[,-1]
prob_train <- model_lasso %>%  predict(newx = x_train)
pred_class_train <- ifelse(prob_train > 0.5, 1, 0)
obs_class_train <- lcdfTrn$loan_status
mean(pred_class_train == obs_class_train)

#confusion matrix traning data
conf_mat_train<-confusionMatrix(as.factor(pred_class_train),as.factor(obs_class_train))
conf_mat_train$table

#model accuracy on test data
x.test <- model.matrix(loan_status ~., lcdfTst)[,-1]
prob_test <- model_lasso %>%  predict(newx = x.test)
pred_class_test <- ifelse(prob_test > 0.5, 1, 0)
obs_class_test <- lcdfTst$loan_status
mean(pred_class_test== obs_class_test)

#Confusion Matrix
conf_mat_test<-confusionMatrix(as.factor(pred_class_test),as.factor(obs_class_test))
conf_mat_test$table

#Printing AUC Graph
pr1 <- prediction(prob_test, lcdfTst$loan_status)
prf1 <- performance(pr1, measure = "tpr", x.measure = "fpr")
plot(prf1)
abline(0,1,lwd = 2, lty = 2)

#auc value
auc <- performance(pr1, measure = "auc")
auc <- auc@y.values[[1]]
auc

#A2)fit model to traning data using cv_lasso$lambda.1se i.e λ which gives the most regularized model 
model_lasso_2 <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.1se)

print(model_lasso_2)

#coefficients
coef(model_lasso_2)

#model accuracy on traning data
x_train_2 <- model.matrix(loan_status ~., lcdfTrn)[,-1]
prob_train_2 <- model_lasso_2 %>%  predict(newx = x_train_2)
pred_class_train2 <- ifelse(prob_train_2 > 0.5, 1, 0)
obs_class_train2 <- lcdfTrn$loan_status
mean(pred_class_train2 == obs_class_train2)

#confusion matrix traning data
conf_mat_train2<-confusionMatrix(as.factor(pred_class_train2),as.factor(obs_class_train2))
conf_mat_train2$table

#model accuracy on test data
x.test2 <- model.matrix(loan_status ~., lcdfTst)[,-1]
prob_test2 <- model_lasso_2 %>%  predict(newx = x.test2)
pred_class_test2 <- ifelse(prob_test2 > 0.5, 1, 0)
obs_class_test2 <- lcdfTst$loan_status
mean(pred_class_test2== obs_class_test2)

#Confusion Matrix
conf_mat_test2<-confusionMatrix(as.factor(pred_class_test2),as.factor(obs_class_test2))
conf_mat_test2$table

#AUC Graph
pr2 <- prediction(prob_test2, lcdfTst$loan_status)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
plot(prf2)
abline(0,1,lwd = 2, lty = 2)

#auc value
auc <- performance(pr2, measure = "auc")
auc <- auc@y.values[[1]]
auc

```

#B2) For Ridge, alpha=0:
```{R}

#to find the best lamda using cross validation with penalizing factor Ridge alpha=0
cv.ridge <- cv.glmnet(x, y, data=subset(os_lcdfTrn, select=-c(annRet, purpose, actualTerm, actualReturn)), family="binomial", alpha=0 )
cv.ridge

#plot cross-validation results:
plot(cv.ridge)

#fit model to traning data using cv_lasso$lambda.min i.e λ which gives minimum cross-validated error
mod_ridge <- glmnet(x, y, alpha = 0, family = "binomial",lambda = cv.ridge$lambda.min)

## Display regression coefficients
coef(mod_ridge)

#model accuracy on traning data
x_train3 <- model.matrix(loan_status ~., lcdfTrn)[,-1]
prob_train3 <- mod_ridge %>%  predict(newx = x_train3)
pred.class3 <- ifelse(prob_train3 > 0.5, 1, 0)
obs_class_train3 <- lcdfTrn$loan_status
mean(pred.class3 == obs_class_train3)

#Confusion Matrix on train data
conf_mat3<-confusionMatrix(as.factor(pred.class3),as.factor(obs_class_train3))
conf_mat3$table

#model accuracy on test data
x.test3 <- model.matrix(loan_status ~., lcdfTst)[,-1]
prob_test3 <- mod_ridge %>%  predict(newx = x.test3)
pred.class3 <- ifelse(prob_test3 > 0.5, 1, 0)
obs.class3 <- lcdfTst$loan_status
mean(pred.class3 == obs.class3)

#Confusion Matrix on test data
conf_mat3<-confusionMatrix(as.factor(pred.class3),as.factor(obs.class3))
conf_mat3$table

#Printing AUC Value
pr3 <- prediction(prob_test3, lcdfTst$loan_status)
prf3 <- performance(pr3, measure = "tpr", x.measure = "fpr")
plot(prf3)
abline(0,1,lwd = 2, lty = 2)

#auc value
auc3 <- performance(pr3, measure = "auc")
auc3 <- auc3@y.values[[1]]
auc3
```



#2) Develop models to identify loans which provide the best returns. Explain how you define returns? Does it include Lending Club’s service costs?

```{r}

#Splitting into traning and test datsets to predict actual returns.
set.seed(9)
nr<-nrow(loan_data)
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) 
lcdfTrn <- loan_data[trnIndex, ] 
lcdfTst <- loan_data[-trnIndex, ]

#count before sampling
lcdfTrn%>% group_by(loan_status) %>% count()

#Undersampling
us_lcdfTrn<-ovun.sample(loan_status~., data=as.data.frame(lcdfTrn), na.action = na.pass, method="under", p=0.5)$data
#us_lcdfTrn%>%group_by(loan_status)%>% count() #count after undersampling

#oversampling
os_lcdfTrn<-ovun.sample(loan_status~., data=as.data.frame(lcdfTrn), na.action = na.pass, method="over", p=0.5)$data
os_lcdfTrn%>%group_by(loan_status)%>% count() #count after oversampling

#both- undersampling and oversampling
bs_lcdfTrn<-ovun.sample(loan_status~., data=as.data.frame(lcdfTrn), na.action = na.pass, method="both", p=0.5)$data
#bs_lcdfTrn%>%group_by(loan_status)%>% count() #count after sampling



```

#To develop models to identify loans which provide the best returns. 
#1)Random forests to identify returns

```{r}

#randomforest using oversampling
rfModel<-ranger(actualReturn~., data=subset(os_lcdfTrn, select = -c(loan_status, annRet, actualTerm)), num.trees=50, importance='permutation')

#print model
print(rfModel)

#Fit model to training data
rfPredRet_trn<-predict(rfModel, lcdfTrn) 

#error on traning data (RMSE)
sqrt(mean((rfPredRet_trn$predictions-os_lcdfTrn$actualReturn)^2))#error rate

#error on test data (RMSE)
rf_RMSE<-sqrt(mean(((predict(rfModel, lcdfTst))$predictions-lcdfTst$actualReturn)^2))
rf_RMSE

#plot based on train data
plot((predict(rfModel,lcdfTrn))$predictions,lcdfTrn$actualReturn)

#plot based on Test data
plot((predict(rfModel,lcdfTst))$predictions,lcdfTst$actualReturn)

#experimenting with varing the number of trees
#rf with 100 trees
rfModel100<-ranger(actualReturn~., data=subset(os_lcdfTrn, select = -c(loan_status, annRet, actualTerm)), num.trees=100, importance='permutation')

#Fit model to training data
rfPredRet_trn100<-predict(rfModel100, lcdfTrn) 

#error on traning data (RMSE)
sqrt(mean((rfPredRet_trn100$predictions-os_lcdfTrn$actualReturn)^2))#error rate

#error on test data (RMSE)
sqrt(mean(((predict(rfModel100, lcdfTst))$predictions-lcdfTst$actualReturn)^2))


#plot based on train data
plot((predict(rfModel,lcdfTrn))$predictions,lcdfTrn$actualReturn)

#plot based on Test data
plot((predict(rfModel,lcdfTst))$predictions,lcdfTst$actualReturn)

#RF with 200 trees
rfModel200<-ranger(actualReturn~., data=subset(os_lcdfTrn, select = -c(loan_status, annRet, actualTerm)), num.trees=200, importance='permutation')

#Fit model to training data
rfPredRet_trn200<-predict(rfModel200, lcdfTrn) 

#error on traning data (RMSE)
sqrt(mean((rfPredRet_trn200$predictions-os_lcdfTrn$actualReturn)^2))#error rate

#error on test data (RMSE)
sqrt(mean(((predict(rfModel200, lcdfTst))$predictions-lcdfTst$actualReturn)^2))
mean(predict(rfModel200, lcdfTst)$predictions-lcdfTst$actualReturn)

#plot based on train data
plot((predict(rfModel,lcdfTrn))$predictions,lcdfTrn$actualReturn)

#plot based on Test data
plot((predict(rfModel,lcdfTst))$predictions,lcdfTst$actualReturn)


#Performance by deciles on Traning data using rf with 200 trees as this is the best model.
PredRet_Trn_rf<-lcdfTrn%>% select(grade, loan_status, actualReturn, actualTerm, int_rate) %>% mutate(rfPredRet=(predict(rfModel, lcdfTrn))$predictions) 

predRet_Trn<-predRet_Trn%>% mutate(tile=ntile(-predRet, 10)) 

x<-predRet_Trn%>% group_by(tile) %>%summarise(count=n(), avgpredRet=mean(predRet), numDefaults=sum(loan_status=="Charged Off"), avgActRet=mean(actualReturn), minRet=min(actualReturn), maxRet=max(actualReturn), avgTer=mean(actualTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )
#x
#length(x)

knitr::kable(x, col.names=c("Tile", "Count", "Avg.Predicted Return", "No.Of Deafults", "Avg Actual return", "Minimum Return", "Maximum Return", "Average Term", "Total A", "Total B", "Total C", "Total D", "Total E", "Total F"), align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))

#performance by deciles on test data
predRet_Tst<-lcdfTst%>% select(grade, loan_status, actualReturn, actualTerm, int_rate) %>% mutate(predRet=(predict(rfModel, lcdfTst))$predictions) 

predRet_Tst<-predRet_Tst%>% mutate(tile=ntile(-predRet, 10)) 
predRet_Tst

x<-predRet_Tst%>% group_by(tile) %>%summarise(count=n(), avgpredRet=mean(predRet), numDefaults=sum(loan_status=="Charged Off"), avgActRet=mean(actualReturn), minRet=min(actualReturn), maxRet=max(actualReturn), avgTer=mean(actualTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )
#x
#length(x)

knitr::kable(x, col.names=c("Tile", "Count", "Avg.Predicted Return", "No.Of Deafults", "Avg Actual return", "Minimum Return", "Maximum Return", "Average Term", "Total A", "Total B", "Total C", "Total D", "Total E", "Total F"), align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))

```
By the graphs we see that the model provides good prediction on the training data whereas on the test data, we do not see a linear relationship, hence the model is overfitting on the training data.



#GBM Model to predict actual return

#parameter tuning for GBM model to find the best model to predict annual Returns:
```{r}
paramGrid<-expand.grid( treeDepth= c(2, 5), minNodeSize= c(10, 30), bagFraction= c(.5, .8, 1), shrinkage = c(0.001, 0.01, 0.1), bestTree=0, minRMSE= 0, trainerror=0, testerror=0)

paramGrid

for(i in 1:nrow(paramGrid)) {
gbm_paramTune<- gbm(formula=actualReturn~., data=subset(os_lcdfTrn, select=-c(annRet, actualTerm, loan_status)), distribution = 'gaussian', n.trees= 1000, interaction.depth= paramGrid$treeDepth[i], n.minobsinnode= paramGrid$minNodeSize[i], bag.fraction= paramGrid$bagFraction[i], shrinkage = paramGrid$shrinkage[i], train.fraction= 0.7, n.cores=NULL )  #use all available cores
#add best tree and its RMSE to paramGrid 
paramGrid$bestTree[i]<-which.min(gbm_paramTune$valid.error) 
paramGrid$minRMSE[i]<-sqrt(min(gbm_paramTune$valid.error))
paramGrid$trainerror[i]<-sqrt(mean((predict(gbm_paramTune, os_lcdfTrn, type="response")-os_lcdfTrn$actualReturn)^2))
paramGrid$testerror[i]<-sqrt(mean((( predict(gbm_paramTune, lcdfTst, type="response"))-lcdfTst$actualReturn)^2))
}

paramGrid$bestTree
paramGrid$minRMSE

#minimum RMSE:
min(paramGrid$minRMSE)

paramGrid$trainerror

#minimum RMSE on traning data:
min(paramGrid$trainerror)

#minimum error on test data
paramGrid$testerror
min(paramGrid$testerror)
```


#Recreating the GBM Model that had the least RMSE error rate

```{r}
#GBM model on oversampled data
gbm_Ret<-gbm(formula=actualReturn~.,data=subset(os_lcdfTrn,select=-c(loan_status,annRet,actualTerm)),distribution="gaussian", n.trees = 200, shrinkage=0.1, bag.fraction = 0.8, n.minobsinnode=30, interaction.depth= 2, cv.folds=5)
gbm_Ret$valid.error
             
print(gbm_Ret)

gbm_PredRet_trn<-predict.gbm(gbm_Ret, os_lcdfTrn, type="response")


#error on training data (RMSE)
sqrt(mean((gbm_PredRet_trn-os_lcdfTrn$actualReturn)^2))

#error on testing data (RMSE)
sqrt(mean((( predict(gbm_Ret, lcdfTst, type="response"))-lcdfTst$actualReturn)^2))

plot((predict(gbm_Ret,lcdfTst)),lcdfTst$actualReturn)
plot((predict(gbm_Ret, os_lcdfTrn)), os_lcdfTrn$actualReturn)

#performance by deciles for traning data:
PredRet_Trn_gbm<-os_lcdfTrn%>% select(grade, loan_status, actualReturn, actualTerm, int_rate) %>% mutate(predRet=(predict.gbm(gbm_Ret, os_lcdfTrn, type="response")))

predRet_Trn<-predRet_Trn%>% mutate(tile=ntile(-predRet, 10)) 

x<-predRet_Trn%>% group_by(tile) %>%summarise(count=n(), avgpredRet=mean(predRet), numDefaults=sum(loan_status=="Charged Off"), avgActRet=mean(actualReturn), minRet=min(actualReturn), maxRet=max(actualReturn), avgTer=mean(actualTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )
#x
#length(x)

knitr::kable(x, col.names=c("Tile", "Count", "Avg.Predicted Return", "No.Of Deafults", "Avg Actual return", "Minimum Return", "Maximum Return", "Average Term", "Total A", "Total B", "Total C", "Total D", "Total E", "Total F"), align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))


#Performance by deciles on test data
predRet_Tst<-lcdfTst%>%select(grade, loan_status, actualReturn, actualTerm, int_rate) %>% mutate(predRet=(predict.gbm(gbm_Ret, lcdfTst, type="response")))

predRet_Tst<-predRet_Tst%>% mutate(tile=ntile(-predRet, 10)) 
predRet_Tst

x<-predRet_Tst%>% group_by(tile) %>%summarise(count=n(), avgpredRet=mean(predRet), numDefaults=sum(loan_status=="Charged Off"), avgActRet=mean(actualReturn), minRet=min(actualReturn), maxRet=max(actualReturn), avgTer=mean(actualTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )

knitr::kable(x, col.names=c("Tile", "Count", "Avg.Predicted Return", "No.Of Deafults", "Avg Actual return", "Minimum Return", "Maximum Return", "Average Term", "Total A", "Total B", "Total C", "Total D", "Total E", "Total F"), align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))


```


#GLM Model to predict annual returns
```{r}

#Using Lasso
xD<-os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)

#using Lasso as penalising factor
Ret_Lasso<-cv.glmnet(data.matrix(xD), os_lcdfTrn$actualReturn , family="gaussian", alpha=1)
Ret_Lasso

#plot cross-validation results:
plot(Ret_Lasso)

#aplying model on traning data using lambda.min
predRet=  predict(Ret_Lasso, data.matrix(os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Lasso$lambda.min, alpha=1 )

#RMSE on traning data
sqrt(mean((os_lcdfTrn$actualReturn-predRet)^2))

#applying model on test data
predRet_test<-predict(Ret_Lasso, data.matrix(lcdfTst%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Lasso$lambda.min, alpha=1 )

#RMSE on test
sqrt(mean((lcdfTst$actualReturn-predRet)^2))

#applying model on traning data using lambda.1se
predRet=  predict(Ret_Lasso, data.matrix(os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Lasso$lambda.1se, alpha=1 )

#RMSE on traning data
sqrt(mean((os_lcdfTrn$actualReturn-predRet)^2))

#applying model on test data
predRet_test<-predict(Ret_Lasso, data.matrix(lcdfTst%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Lasso$lambda.1se, alpha=1 )

#RMSE on test
sqrt(mean((lcdfTst$actualReturn-predRet)^2))


#Repeat using ridge

xD<-os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)

#using Lasso as penalising factor
Ret_Ridge<-cv.glmnet(data.matrix(xD), os_lcdfTrn$actualReturn , family="gaussian", alpha=0)
Ret_Ridge

#plot cross-validation results:
plot(Ret_Ridge)

#aplying model on traning data using lambda.min
predRet=  predict(Ret_Ridge, data.matrix(os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Ridge$lambda.min, alpha=0 )

#RMSE on traning data
sqrt(mean((os_lcdfTrn$actualReturn-predRet)^2))

#applying model on test data
predRet_test<-predict(Ret_Ridge, data.matrix(lcdfTst%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Ridge$lambda.min, alpha=0 )

#RMSE on test
sqrt(mean((lcdfTst$actualReturn-predRet)^2))

#applying model on traning data using lambda.1se
predRet=  predict(Ret_Ridge, data.matrix(os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Ridge$lambda.1se, alpha=0 )

#RMSE on traning data
sqrt(mean((os_lcdfTrn$actualReturn-predRet)^2))

#applying model on test data
predRet_test<-predict(Ret_Ridge, data.matrix(lcdfTst%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Ridge$lambda.1se, alpha=0 )

#RMSE on test
sqrt(mean((lcdfTst$actualReturn-predRet)^2))

#prediction on traning data
predRet_Trn<-os_lcdfTrn%>% select(grade, loan_status, actualReturn, actualTerm, int_rate) %>% mutate(predRet=  predict(Ret_Lasso, data.matrix(os_lcdfTrn%>% select(-loan_status, -actualTerm, -annRet, -actualReturn)), s=Ret_Lasso$lambda.min, alpha=1 ) )

sqrt(mean((predict(Ret_Lasso, os_lcdfTrn, type="response")-os_lcdfTrn$actualReturn)^2))

predRet_Trn
predRet_Trn<-predRet_Trn%>% mutate(tile=ntile(-predRet, 10))

#table 
predRet_Trn%>% group_by(tile) %>%  summarise(count=n(), avgpredRet=mean(predRet), numDefaults=sum(loan_status=="Charged Off"), avgActRet=mean(actualReturn), minRet=min(actualReturn), maxRet=max(actualReturn), avgTer=mean(actualTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )

knitr::kable(x, col.names=c("Tile", "Count", "Avg.Predicted Return", "No.Of Deafults", "Avg Actual return", "Minimum Return", "Maximum Return", "Average Term", "Total A", "Total B", "Total C", "Total D", "Total E", "Total F"), align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))

```


